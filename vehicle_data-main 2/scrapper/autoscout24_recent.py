import time
from typing import List, Dict, Any, Optional
import json
import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass
from datetime import datetime
from utils.key_mapping import convert_vehicle_data
from utils.filters import *
import threading
from proxies.webshare_optimized import WEBSHARE
from database.db import VehicleDatabase
from logger.logger_setup import LoggerSetup
from configuration.config import Config
from concurrent.futures import ThreadPoolExecutor, as_completed

@dataclass
class ScraperConfig:
    """Configuration for the hourly scraper"""
    max_pages: int = 2000  # Increased for ultra-high throughput
    max_retries: int = 1  # Minimal retries for maximum speed
    delay_between_requests: float = 0.01  # Ultra-minimal delay


@dataclass
class ScraperStats:
    """Track scraper statistics"""
    total_listings: int = 0
    total_requests: int = 0
    failed_requests: int = 0
    pages_processed: int = 0
    duplicates_skipped: int = 0
    list_process_per_page: int = 0
    consective_no_data_page_count: int = 0


class AutoScout24HourlyScraper:
    """Hourly scraper for AutoScout24 - fetches latest listings sorted by age"""

    def __init__(self, config: Optional[ScraperConfig] = None):
        """Initialize scraper with configuration"""
        self.config = config or ScraperConfig()
        self.stats = ScraperStats()
        self.log = LoggerSetup("autoscout24_recent.log").get_logger()
        self.webshare_obj = WEBSHARE()
        self.unique_features = autoscout24_features
        self.db_obj = VehicleDatabase(logger=self.log)
        self.thread_limit = Config.AUTOSCOUT_THREAD_COUNT

    def _make_request(self, url: str, params: Optional[Dict[str, Any]] = None,
                      is_pagination: bool = False) -> Optional[requests.Response]:
        """Make HTTP request with retry logic and error handling"""
        for attempt in range(self.config.max_retries):
            try:
                if is_pagination:
                    headers = {
                        "accept": "*/*",
                        "accept-language": "en-PK,en;q=0.9,ur-PK;q=0.8,ur;q=0.7,en-GB;q=0.6,en-US;q=0.5",
                        "priority": "u=1, i",
                        "referer": url,
                        "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
                        "sec-ch-ua-mobile": "?0",
                        "sec-ch-ua-platform": '"Windows"',
                        "sec-fetch-dest": "empty",
                        "sec-fetch-mode": "cors",
                        "sec-fetch-site": "same-origin",
                        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        "x-nextjs-data": "1"
                    }
                else:
                    headers = {
                        "Upgrade-Insecure-Requests": "1",
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
                        "sec-ch-ua-mobile": "?0",
                        "sec-ch-ua-platform": '"Windows"'
                    }

                # Use high-concurrency proxy with unique session per request
                proxy = self.webshare_obj.get_high_concurrency_proxy(thread_id=threading.get_ident())
                response = requests.get(url, params=params, headers=headers,
                                        proxies=proxy, timeout=5)
                self.stats.total_requests += 1

                if response.status_code == 200:
                    return response
                else:
                    self.log.info(f"‚ö†Ô∏è  HTTP {response.status_code} on attempt {attempt + 1}/{self.config.max_retries}")

            except requests.exceptions.Timeout:
                self.log.error(f"‚è±Ô∏è  Timeout on attempt {attempt + 1}/{self.config.max_retries}")
            except requests.exceptions.ConnectionError:
                self.log.error(f"üîå Connection error on attempt {attempt + 1}/{self.config.max_retries}")
            except Exception as e:
                self.log.error(f"‚ùå Error on attempt {attempt + 1}/{self.config.max_retries}: {str(e)[:100]}")

            if attempt < self.config.max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff

        self.stats.failed_requests += 1
        return None

    def get_pagination_response(self, url: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Get search results with parameters"""
        response = self._make_request(url, params=params, is_pagination=True)

        if response:
            try:
                return response.json()
            except json.JSONDecodeError:
                self.log.error("‚ùå Failed to parse JSON response")
                return None
        return None

    def get_detail_response(self, url: str) -> Optional[Dict[str, Any]]:
        """Get product detail page and extract JSON (__NEXT_DATA__) using BeautifulSoup"""
        response = self._make_request(url, is_pagination=False)

        if not response:
            return None

        try:
            soup = BeautifulSoup(response.text, "html.parser")

            # Find the <script> tag with id="__NEXT_DATA__"
            script_tag = soup.find("script", id="__NEXT_DATA__")
            if not script_tag:
                self.log.info("‚ö†Ô∏è No <script id='__NEXT_DATA__'> found on page")
                return None

            # Get the script content
            script_content = script_tag.string or script_tag.get_text()
            if not script_content:
                self.log.info("‚ö†Ô∏è Script tag found but content is empty")
                return None

            # Parse JSON
            return json.loads(script_content)

        except json.JSONDecodeError as e:
            self.log.error(f"‚ùå JSON decode error: {str(e)[:100]}")
            return None
        except Exception as e:
            self.log.error(f"‚ùå Error parsing detail page: {str(e)[:100]}")
            return None

    def parse_listing(self, data: dict) -> dict:
        """Parse individual listing data"""
        parsed = {}
        try:
            # ID and URL
            parsed["id"] = data.get("id")
            parsed["url"] = data.get("url")

            # Price
            price = data.get("price", {}).get("priceFormatted")
            if price:
                parsed["price"] = price

            # Images
            images = data.get("images")
            if isinstance(images, list) and images:
                parsed["images"] = json.dumps(images, ensure_ascii=False)
            else:
                parsed["images"] = json.dumps([])

            # Vehicle details
            vehicle = data.get("vehicle", {})
            for key, value in vehicle.items():
                if value not in [None, "", [], {}]:
                    parsed[f"vehicle_{key}"] = value

            # Location
            loc = data.get("location", {})
            for key, value in loc.items():
                if value not in [None, "", [], {}]:
                    parsed[f"location_{key}"] = value

            # Seller
            seller = data.get("seller", {}).get('contactName', '')
            parsed['seller_name'] = seller

            # Tracking
            tracking = data.get("tracking", {})
            for key, value in tracking.items():
                if value not in [None, "", [], {}]:
                    parsed[f"tracking_{key}"] = value

            # Tracking Parameters
            for param in data.get("trackingParameters", []):
                key = param.get("key")
                value = param.get("value")
                if key and value not in [None, "", [], {}]:
                    parsed[f"tracking_{key}"] = value

            # Vehicle Details with translations
            translations = {
                "Kilometerstand": "mileage",
                "Getriebe": "transmission",
                "Erstzulassung": "first_registration",
                "Kraftstoff": "fuel",
                "Leistung": "power",
                "Kraftstoffverbrauch": "fuel_consumption",
                "CO‚ÇÇ-Emissionen": "co2_emission",
            }

            for item in data.get("vehicleDetails", []):
                label = item.get("ariaLabel")
                value = item.get("data")
                if label and value not in [None, "", [], {}]:
                    label_en = translations.get(label, label)
                    parsed[f"vehicle_detail_{label_en.replace(' ', '_').lower()}"] = value

            # Clean up
            parsed = {k: v for k, v in parsed.items()
                      if v not in [None, "", [], {}, "N/A", "unknown"]}

            return parsed
        except Exception as e:
            self.log.error(f"‚ùå Error parsing listing: {e}")
            return {}

    def parse_detail_listing(self, basic_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Parse detailed listing data from product page"""
        listing_id = basic_data.get('id')

        # Check for duplicate
        if self.db_obj.check_id_exists(listing_id, 'autoscout24'):
            self.log.info(f"‚è≠Ô∏è  Skipping duplicate ID: {listing_id}")
            self.stats.duplicates_skipped += 1
            return None

        try:
            # Construct full URL
            url = 'https://www.autoscout24.de' + basic_data.get('url', '')
            basic_data['url'] = url

            product_response = self.get_detail_response(url)

            if not product_response:
                self.log.info(f"‚ö†Ô∏è  Failed to get details for: {url}")
                return basic_data

            # Extract description
            try:
                description = product_response['props']['pageProps']['listingDetails']['description']
                if description:
                    soup = BeautifulSoup(description, "html.parser")
                    clean_text = soup.get_text().strip()
                    basic_data['description'] = clean_text
                else:
                    basic_data['description'] = ''
            except:
                basic_data['description'] = ''

            try:
                features = [p['id']['formatted'] for p in
                            product_response['props']['pageProps']['listingDetails']['vehicle']['rawData']['equipment'][
                                'as24']]
            except:
                features = []
            self.unique_features.update(features)

            new_data = {}
            for feature in self.unique_features:
                new_data[feature] = feature in features

            try:
                for key, value in product_response['props']['pageProps']['listingDetails']['vehicle'].items():
                    if isinstance(value, (type(None), str, int, float, bool)):
                        new_data[key] = value
            except:
                pass

            try:
                for key, value in product_response['props']['pageProps']['listingDetails']['vehicle'].items():
                    if isinstance(value, dict) and 'formatted' in value:
                        new_data[key] = value['formatted']
            except:
                pass

            try:
                for key, value in product_response['props']['pageProps']['listingDetails']['vehicle']['wltp'].items():
                    if value:
                        new_data[key] = value['formatted']
                    else:
                        new_data[key] = None
            except:
                pass

            try:
                for key, value in product_response['props']['pageProps']['listingDetails']['vehicle'][
                    'costModel'].items():
                    if value:
                        new_data[key] = value
            except:
                pass

            try:
                new_data['price_text'] = product_response['props']['pageProps']['listingDetails']['prices']['error'][
                    'text']
            except:
                pass

            try:
                new_data['identifier'] = product_response['props']['pageProps']['listingDetails']['identifier'][
                    'offerReference']
            except:
                pass

            basic_data.update(new_data)
            self.log.info(f"‚úÖ Parsed: {basic_data.get('url', 'Unknown')[:50]} - ‚Ç¨{basic_data.get('price', 'N/A')}")
            return basic_data

        except Exception as e:
            self.log.error(f"‚ùå Error parsing details for {basic_data.get('url', 'Unknown')}: {str(e)[:100]}")
            return basic_data

    def process_listings(self, listings: List[Dict[str, Any]]):
        """Process multiple listings concurrently"""
        lock = threading.Lock()

        def process_single(listing):
            try:
                basic_data = self.parse_listing(listing)
                if not basic_data:
                    return

                detailed_data = self.parse_detail_listing(basic_data)
                if not detailed_data:
                    return

                # Build title
                detailed_data[
                    'title'] = f"{detailed_data.get('vehicle_make', '')} {detailed_data.get('vehicle_model', '')} {detailed_data.get('vehicle_modelVersionInput', '')}".strip()

                final_data = convert_vehicle_data(detailed_data, 'autoscout24')

                # Thread-safe append and counter increment
                with lock:
                    self.db_obj.insert_vehicle(final_data)
                    self.stats.total_listings += 1
                    self.stats.list_process_per_page += 1

            except Exception as e:
                self.log.error(f"‚ùå Error processing listing: {e}")

        with ThreadPoolExecutor(max_workers=self.thread_limit) as executor:
            # Submit all tasks and collect futures
            futures = [executor.submit(process_single, listing) for listing in listings]

            # Collect results as they complete
            for future in as_completed(futures):
                future.result()

    def run(self):
        """Main execution method - fetch latest listings sorted by age"""
        self.log.info("üöÄ Starting AutoScout24 hourly scraping...")
        self.log.info(f"‚öôÔ∏è  Config: Max {self.config.max_pages} pages, sorted by age")

        start_time = time.time()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.log.info(f"üïê Run timestamp: {timestamp}")

        url = "https://www.autoscout24.de/_next/data/as24-search-funnel_main-20250924171425/lst.json"
        page_number = 1

        try:
            while page_number < self.config.max_pages:
                self.log.info(f"\n{'=' * 60}")
                self.log.info(f"üìñ Processing page {page_number}")

                # Build search parameters
                params = {
                    "atype": "C",
                    "cy": "D",
                    "damaged_listing": "exclude",
                    "desc": "1",
                    "ocs_listing": "include",
                    "powertype": "kw",
                    "search_id": "fgm1i1ycu0",
                    "sort": "age",  # Sort by age (newest first)
                    "source": "listpage_pagination",
                    "ustate": "N,U",
                    "page": str(page_number)
                }

                response = self.get_pagination_response(url, params)

                if not response or 'pageProps' not in response:
                    self.log.info(f"‚ùå Failed to get response for page {page_number}")
                    break

                page_props = response['pageProps']
                num_results = page_props.get('numberOfResults', 0)
                num_pages = page_props.get('numberOfPages', 0)

                if page_number == 1:
                    self.log.info(f"üìà Total results available: {num_results}")
                    self.log.info(f"üìÑ Total pages available: {num_pages}")

                # Extract listings
                listings = page_props.get('listings', [])

                if not listings:
                    self.log.info("‚ö†Ô∏è  No listings found on this page")
                    break

                self.log.info(f"üîÑ Processing {len(listings)} listings from this page")

                # Process listings
                self.process_listings(listings)
                self.stats.pages_processed += 1

                self.log.info(f"‚úÖ Parsed {self.stats.list_process_per_page} listings (Total: {self.stats.total_listings})")
                if self.stats.list_process_per_page == 0:
                    self.stats.consective_no_data_page_count += 1
                else:
                    self.stats.list_process_per_page = 0
                    self.stats.consective_no_data_page_count = 0

                if self.stats.consective_no_data_page_count == 3:
                    self.log.info(f"üìÑ Three pages have no New data Stopping script!")
                    break

                if page_number >= num_pages:
                    self.log.info(f"üìÑ Reached last page ({num_pages})")
                    break

                page_number += 1

                # Add delay between pages
                time.sleep(self.config.delay_between_requests)

        except KeyboardInterrupt:
            self.log.error("\n\n‚ö†Ô∏è  Scraping interrupted by user")
        except Exception as e:
            self.log.error(f"‚ùå Error during scraping: {str(e)[:200]}")

        elapsed_time = time.time() - start_time

        # self.log. final statistics
        self.log.info(f"\n{'=' * 60}")
        self.log.info("üìä SCRAPING COMPLETED")
        self.log.info(f"{'=' * 60}")
        self.log.info(f"‚úÖ Total listings collected: {self.stats.total_listings}")
        self.log.info(f"‚è≠Ô∏è  Duplicates skipped: {self.stats.duplicates_skipped}")
        self.log.info(f"üìÑ Pages processed: {self.stats.pages_processed}")
        self.log.info(f"üåê Total requests: {self.stats.total_requests}")
        self.log.info(f"‚ùå Failed requests: {self.stats.failed_requests}")
        self.log.info(f"‚è±Ô∏è  Time elapsed: {elapsed_time:.2f} seconds")
        if elapsed_time > 0:
            self.log.info(f"‚ö° Average: {self.stats.total_listings / elapsed_time:.2f} listings/sec")
        self.log.info(f"{'=' * 60}")


def main():
    """Entry point for the hourly scraper"""
    # Create configuration
    config = ScraperConfig(
        max_pages=200,
        max_retries=3,
        delay_between_requests=1.0
    )

    # Initialize and run scraper
    scraper = AutoScout24HourlyScraper(config)
    scraper.run()
